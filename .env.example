# =============================================================================
# AI Agent IRPF - Environment Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Google AI Configuration (Gemini 2.5 Pro - Primary Model)
# -----------------------------------------------------------------------------
# Obtain your API key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# -----------------------------------------------------------------------------
# Google Cloud Configuration (Claude Sonnet 4 via Vertex AI - Fallback Model)
# -----------------------------------------------------------------------------
# Project ID from Google Cloud Console
GOOGLE_CLOUD_PROJECT=your_project_id_here

# Region for Vertex AI (recommended: us-central1)
GOOGLE_CLOUD_LOCATION=us-central1

# Path to service account key file (optional if using default credentials)
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account-key.json

# -----------------------------------------------------------------------------
# Application Configuration
# -----------------------------------------------------------------------------
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Maximum file size for processing (in MB)
MAX_FILE_SIZE_MB=50

# Maximum number of records to process in batch
MAX_BATCH_SIZE=1000

# Enable/disable automatic backup before modifications
AUTO_BACKUP=true

# Backup retention days
BACKUP_RETENTION_DAYS=30

# -----------------------------------------------------------------------------
# OCR Configuration
# -----------------------------------------------------------------------------
# Tesseract language (for OCR processing)
TESSERACT_LANG=por

# OCR confidence threshold (0-100)
OCR_CONFIDENCE_THRESHOLD=60

# -----------------------------------------------------------------------------
# Agent Configuration
# -----------------------------------------------------------------------------
# Model selection: gemini, claude, auto (auto will fallback)
PREFERRED_MODEL=auto

# Agent temperature (0.0-1.0, lower = more deterministic)
AGENT_TEMPERATURE=0.1

# Maximum tokens for agent responses
MAX_OUTPUT_TOKENS=4000

# Maximum iterations for agent reasoning
MAX_AGENT_ITERATIONS=5

# Enable verbose agent output
AGENT_VERBOSE=true

# -----------------------------------------------------------------------------
# File Processing Configuration
# -----------------------------------------------------------------------------
# Default encoding for DBK files
DBK_ENCODING=latin-1

# Enable checksum validation on file read
VALIDATE_CHECKSUMS=true

# Auto-fix checksums when writing files
AUTO_FIX_CHECKSUMS=true

# -----------------------------------------------------------------------------
# Security Configuration
# -----------------------------------------------------------------------------
# Enable file operation restrictions
RESTRICT_FILE_OPERATIONS=true

# Allowed file extensions for processing (comma-separated)
ALLOWED_EXTENSIONS=.dbk,.pdf,.png,.jpg,.jpeg

# Maximum conversation history to keep in memory
MAX_CONVERSATION_HISTORY=10

# -----------------------------------------------------------------------------
# Development Configuration
# -----------------------------------------------------------------------------
# Enable development mode (more verbose logging, mock data)
DEV_MODE=false

# Use mock data when external services are unavailable
USE_MOCK_DATA=false

# Enable debug mode for LangChain agent
LANGCHAIN_DEBUG=false

# Enable tracing for LangChain operations
LANGCHAIN_TRACING_V2=false

# LangSmith API key (for tracing, optional)
LANGCHAIN_API_KEY=your_langsmith_api_key_here

# -----------------------------------------------------------------------------
# Performance Configuration
# -----------------------------------------------------------------------------
# Number of concurrent workers for file processing
WORKER_THREADS=4

# Timeout for AI model requests (seconds)
MODEL_TIMEOUT=60

# Cache TTL for search results (seconds)
SEARCH_CACHE_TTL=3600

# Enable async processing where possible
ENABLE_ASYNC=true
